---
title: "kernels"
author: "Marc Vernet and Jordi Puig"
date: "11/28/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd(".")
```

## Load the data

```{r}
gen_raw = read.csv("data/cannabis_gen.csv")
head(gen_raw)
```

Now we separate the ID
```{r}
gen = gen_raw[2:175]
X = gen[1:173]
```

### Prepare folds for k-CV

```{r}
k = 10
N = dim(gen)[1]
k.folds = sample(rep(1:k, length=N), N, replace=FALSE)
```


## Prepare different models
```{r}
library(e1071) # module for svm()
library(kernlab)
```

```{r}
# exemple de custom kernel
# Jaccard similarity
kp=function(d,e){
  intersection = length(intersect(d,e))
  union = 2*length(d) - intersection
  return (intersection / union)
}

class(kp)="kernel"
```


```{r}
train.svm.kCV <- function (dataset, which.kernel, mycost, folds)
{
  valid.error <- rep(0,k)
  for (i in 1:k)
  #for (i in 1:1) 
  {  
    train <- dataset[folds!=i,] # for building the model (training)
    valid <- dataset[folds==i,] # for prediction (validation)
    
    x_train <- train[,1:173]
    t_train <- train[,174]
    
    #model <- svm(x_train, t_train, type="C-classification", cost=mycost, kernel=which.kernel, scale = FALSE)
    # funcio per custom kernels
    model <- ksvm(as.matrix(x_train), as.factor(t_train), type="C-svc", cost=mycost, kernel=which.kernel, scale = FALSE)
    
    x_valid <- valid[,1:173]
    pred <- predict(model,x_valid)
    t_true <- valid[,174]
    
    # compute validation error for part 'i'
    valid.error[i] <- sum(pred != t_true)/length(t_true)
  }
  # return average validation error
  sum(valid.error)/length(valid.error)
}
```

Try if the function works
```{r}
c = 1
train.svm.kCV(gen, kp, c, k.folds)
```

### To do

We need a function to optimize the `cost` parameter (with kCV for example).

We need to choose the best kernel for the data.

### Idees de recerca

Calcular matrius de kernel amb diferents funcions i fer heatmaps per visualitzar

```{r}
kp=function(d,e){
  intersection = length(intersect(d,e))
  union = 2*length(d) - intersection
  return (intersection / union)
}

class(kp)="kernel"

some.kernel = function(d, e) {
  n = length(d)
  sim = 0
  for (i in c(1:n)) {
    if (d[i] == e[i]) {
      sim = sim + d[i]
    }
  }
  sum = sum(d)
  return(sim/sum)
}
class(some.kernel) = "kernel"
```

```{r}
c = 1
train.svm.kCV(gen, some.kernel, c, k.folds)
```


```{r}
compute.kernel.matrix = function(dd, kernel) {
  n  = dim(dd)[1]
  k.matrix = matrix(nrow = n, ncol = n)
  for (i in c(1:n)) {
    for (j in c(1:n)) {
      k.matrix[i,j] = kernel(dd[i,], dd[j,])
    }
  }
  return(k.matrix)
}

jaccard = compute.kernel.matrix(X, kp)
```

```{r}
heatmap(jaccard)
```



Comprovar si amb kernel methods i utilitzant un conjunt de training més petit es pot obtindre bons resultats

Comparació del temps d'execució

Applicar el kernel per missing data que ens va explicar

Provar d'aplicar un kernel PCA i compararlo amb el PCA normal

